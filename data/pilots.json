[
  {
    "id": "greenlight-traffic-optimizer",
    "title": "GreenLight Traffic Signal Optimizer",
    "sector": "Transportation",
    "tags": [
      "adaptive-signals",
      "traffic",
      "signal-timing",
      "corridor-optimization",
      "emissions",
      "V2I",
      "arterial"
    ],
    "oneLiner": "Use AI to dynamically optimize traffic light timings across intersections to reduce congestion and emissions",
    "stack": [
      "Python",
      "FastAPI",
      "PyTorch",
      "SUMO",
      "PostgreSQL",
      "Kafka"
    ],
    "kpis": [
      "25% reduction in intersection delays",
      "Lower vehicle idle time",
      "Improved travel time reliability"
    ],
    "feasibility": "solo-90-day",
    "wheelRisk": 5,
    "problem": "City traffic signals often operate on fixed schedules that fail to adapt to real-time conditions. This leads to unnecessary congestion, longer travel times, and higher emissions as vehicles idle at red lights.",
    "approach": "Deploy an AI-driven system that ingests live traffic data (cameras, sensors) and optimizes signal timing in real-time. Use machine learning to adapt to patterns throughout the day. Start with one corridor of connected intersections as a pilot and measure improvements in flow and emissions.",
    "oneWeek": "Scope Sprint: Collect baseline traffic data for a busy intersection corridor. Simulate traffic with existing timings vs. optimized timing in software (e.g., SUMO simulator). Develop a simple ML model that adjusts one intersection’s timing based on flow.",
    "ninetyDay": "Deploy a prototype on a small network of intersections. Integrate with traffic controller hardware or simulator. Implement a dashboard to monitor live traffic conditions and signal states. Demonstrate reduced average wait times and coordinate with city traffic engineers for feedback.",
    "risks": [
      "Integration with legacy signal hardware",
      "Unexpected traffic patterns causing suboptimal AI decisions",
      "Safety and compliance with traffic regulations"
    ],
    "buyVsBuild": "Consider: Surtrac (CMU spinoff) or Miovision for adaptive signals. These are proven but costly. Build if needing a tailored solution for specific city conditions or to integrate with custom traffic management systems.",
    "emb": [
      0.6,
      0.4,
      0.5,
      0.2,
      0.7,
      0.4,
      0.2,
      0.2
    ]
  },
  {
    "id": "voltspot-ev-planner",
    "title": "VoltSpot EV Charging Planner",
    "sector": "Transportation",
    "tags": [
      "EV",
      "GIS",
      "planning",
      "grid-capacity",
      "equity",
      "utilization"
    ],
    "oneLiner": "Map-based tool to identify optimal locations for new EV charging stations using traffic, grid capacity and demographic data",
    "stack": [
      "Next.js",
      "PostgreSQL/PostGIS",
      "Mapbox GL",
      "Python",
      "Turf.js"
    ],
    "kpis": [
      "50% faster site planning",
      "Higher charger utilization rates",
      "Coverage gaps identified and reduced"
    ],
    "feasibility": "solo-90-day",
    "wheelRisk": 4,
    "problem": "Transportation agencies and utilities need to rapidly expand EV charging networks but struggle to decide where stations will be most effective. Planners must manually combine traffic patterns, electric grid data, and community needs, which is time-consuming and prone to oversight.",
    "approach": "Build a web application that layers relevant datasets on an interactive map. Include traffic volumes, current EV ownership, power grid capacity, and points of interest. Implement an algorithm to score potential sites based on criteria like demand and infrastructure readiness. The tool will output top recommended locations and allow planners to adjust assumptions.",
    "oneWeek": "Scope Sprint: Gather open data for one city (traffic counts, existing chargers, population). Create a basic map UI with Mapbox showing current charger locations and overlay traffic density. Outline scoring criteria for site selection.",
    "ninetyDay": "Develop data ingests for multiple datasets (grid substation locations, demographics, etc.). Implement the scoring algorithm and visualization of high-potential areas. Add CSV import for custom data. Pilot the tool with a city planning team to refine recommendations and interface usability.",
    "risks": [
      "Data availability and accuracy (especially grid data)",
      "Weighting criteria appropriately in the algorithm",
      "Stakeholder buy-in on suggested sites"
    ],
    "buyVsBuild": "Consider: using existing GIS planning tools or consulting reports for EV infrastructure. However, those can be static or generic. Build for a customizable, interactive tool tailored to specific regional data and to update as conditions change.",
    "emb": [
      0.8,
      0.9,
      0.6,
      0.2,
      0.9,
      0.6,
      0.4,
      0.5
    ]
  },
  {
    "id": "pavewatch-road-monitor",
    "title": "PaveWatch Road Condition Monitor",
    "sector": "Transportation",
    "tags": [
      "pavement",
      "telemetry",
      "asset-management",
      "roads",
      "anomaly-detection"
    ],
    "oneLiner": "Collect and analyze road condition data via sensors and vehicles to predict maintenance needs and improve safety",
    "stack": [
      "Python",
      "AWS IoT Core",
      "AWS Lambda",
      "Amazon S3",
      "DynamoDB"
    ],
    "kpis": [
      "20% reduction in inspection costs",
      "Earlier pothole detection (by 1 month)",
      "Lower vehicle damage claims"
    ],
    "feasibility": "solo-90-day",
    "wheelRisk": 5,
    "problem": "Roadway maintenance is often reactive, relying on periodic inspections or public complaints to find potholes and cracks. This reactive approach can lead to worsening damage and higher repair costs. A proactive system is needed to continuously monitor pavement health and flag issues early.",
    "approach": "Deploy accelerometer and vibration sensors on fleet vehicles or dedicated IoT devices on road segments to capture real-time condition data. Transmit data to a cloud platform for analysis of roughness and anomalies. Use machine learning to identify patterns that indicate developing potholes or structural issues, allowing maintenance crews to intervene sooner.",
    "oneWeek": "Scope Sprint: Instrument one vehicle with a smartphone or sensor to collect road vibration data on a known route. Develop a simple script to identify spikes in vibration (potential potholes) and map those locations. Validate findings against actual road conditions.",
    "ninetyDay": "Scale up to a small fleet or multiple road sections. Build a cloud data pipeline with AWS IoT for real-time data ingestion. Implement a dashboard (Power BI or web) highlighting road condition indices and hot spots. Integrate a scheduling feature to notify maintenance teams when certain thresholds are exceeded, demonstrating a closed-loop system.",
    "risks": [
      "Data noise from vehicle types and driving behavior",
      "Sensor maintenance and durability on vehicles/road",
      "Integration with existing asset management systems"
    ],
    "buyVsBuild": "Consider: commercial offerings like RoadBotics (AI-based road assessment via smartphone video) or StreetScan. These provide turn-key road surveys. Build if an ongoing, in-house monitoring network is preferred for continuous data and integration with local operations.",
    "emb": [
      0.3,
      1.0,
      0.9,
      0.8,
      0.0,
      0.8,
      0.6,
      0.6
    ]
  },
  {
    "id": "aquawatch-quality-network",
    "title": "AquaWatch Water Quality Network",
    "sector": "Water",
    "tags": [
      "water-quality",
      "real-time",
      "SCADA",
      "alerts",
      "compliance"
    ],
    "oneLiner": "Real-time water quality monitoring system using IoT sensors for early contamination detection and alerts",
    "stack": [
      "Python",
      "Azure IoT Hub",
      "InfluxDB",
      "Grafana"
    ],
    "kpis": [
      "Continuous monitoring (vs weekly manual tests)",
      "Alerts triggered within minutes of threshold breach",
      "Improved compliance reporting efficiency"
    ],
    "feasibility": "solo-90-day",
    "wheelRisk": 4,
    "problem": "Utilities and environmental agencies rely on periodic manual water sampling, which can miss sudden contamination events. Delayed detection of water quality issues can pose health risks and incur regulatory penalties. A faster, automated monitoring approach is needed to ensure water safety.",
    "approach": "Deploy a network of IoT sensor stations at critical points (e.g., treatment plant output, distribution nodes, watershed inflows). Each station measures parameters like pH, turbidity, and chemical contaminants, sending data to a central cloud platform in real-time. The platform visualizes trends and triggers alerts when readings exceed safe thresholds, allowing immediate response.",
    "oneWeek": "Scope Sprint: Set up a prototype sensor (Raspberry Pi + basic water quality sensor) in the lab. Stream data to an Azure IoT instance. Create a simple Grafana dashboard showing live pH and turbidity readings. Simulate an anomaly to test alert generation.",
    "ninetyDay": "Roll out 3-5 pilot sensor units in the field (e.g., at a water utility). Develop a full dashboard with location maps and historical trend charts for each sensor. Implement alert notifications via email/SMS for threshold breaches. Work with water quality engineers to calibrate sensor accuracy and validate the system against manual lab results.",
    "risks": [
      "Sensor calibration drift over time",
      "Power and connectivity issues at remote sites",
      "Handling large data volumes and false-positive alerts"
    ],
    "buyVsBuild": "Consider: off-the-shelf solutions like Libelium's Smart Water IoT kit or Hach's remote monitoring systems, which provide hardware and software but may be expensive or inflexible. Build to customize sensor types and integrate with existing SCADA or reporting tools at lower cost.",
    "emb": [
      0.7,
      0.9,
      0.3,
      0.4,
      0.1,
      0.7,
      0.4,
      0.5
    ]
  },
  {
    "id": "hydrocast-flood-predictor",
    "title": "HydroCast Flood Predictor",
    "sector": "Water",
    "tags": [
      "flood-forecasting",
      "hydrology",
      "inundation",
      "early-warning",
      "climate-risk",
      "DEM"
    ],
    "oneLiner": "Predicts flood extents and risk levels using weather forecasts and hydrological modeling to improve emergency preparedness",
    "stack": [
      "Python",
      "TensorFlow",
      "xarray",
      "PostGIS",
      "Leaflet",
      "NOAA/NWS APIs"
    ],
    "kpis": [
      "Improved flood warning lead time (e.g. +48 hours)",
      "High-risk zones identified with >90% accuracy",
      "Emergency response planning time reduced"
    ],
    "feasibility": "solo-90-day",
    "wheelRisk": 6,
    "problem": "Communities face increasing flood risks due to climate change, but existing flood maps are static and forecasts often lack localized detail. Emergency managers need better predictive tools to anticipate where flooding will occur and how severe it will be, so they can allocate resources and issue warnings effectively.",
    "approach": "Combine real-time rainfall forecasts with terrain and river data to simulate potential flood events. Develop an AI model trained on historical flood data to refine predictions of flood extent. Present results on an interactive map showing predicted inundation areas, depths, and timelines. The tool will allow scenario planning (e.g., what-if analysis for different rainfall intensities) for emergency preparedness.",
    "oneWeek": "Scope Sprint: Choose a pilot area (e.g., one river basin or city). Gather elevation data (DEM) and past flood records for that area. Set up a basic hydrologic model or use a simple runoff calculation to estimate flood extent for a known storm. Display this on a Leaflet map as a proof of concept.",
    "ninetyDay": "Integrate live weather forecast APIs. Train a TensorFlow model on historical storm-to-flood outcomes to improve prediction accuracy for the pilot area. Develop the web interface with Leaflet to dynamically display flood risk zones and depth color-coding. Work with local emergency management to test the predictions against minor events and refine the alert thresholds and visualization for clarity.",
    "risks": [
      "Accuracy of models for extreme events",
      "Uncertainty in weather forecasts propagating to predictions",
      "Liability concerns if predictions are used for public warnings"
    ],
    "buyVsBuild": "Consider: established flood modeling software (HEC-RAS, FEMA tools) for rigorous analysis. These require expertise and time for each scenario. Build if aiming for a quicker, automated predictor that can update with real-time data, at the expense of some precision but offering speed and customization (especially useful for localized applications).",
    "emb": [
      0.5,
      1.0,
      0.8,
      0.3,
      0.8,
      1.0,
      0.5,
      0.4
    ]
  },
  {
    "id": "gridsense-microgrid-optimizer",
    "title": "GridSense Microgrid Optimizer",
    "sector": "Energy",
    "tags": [
      "microgrid",
      "optimization",
      "DERMS",
      "battery",
      "peak-shaving",
      "tariff-modeling",
      "reliability"
    ],
    "oneLiner": "AI-driven control platform for microgrids to balance solar, battery, and grid power, optimizing cost and reliability",
    "stack": [
      "Python",
      "Pyomo",
      "TensorFlow",
      "TimescaleDB",
      "Node.js"
    ],
    "kpis": [
      "15% energy cost savings achieved",
      "Higher renewable utilization (up to 80% solar use)",
      "Reduced peak grid demand by 25%"
    ],
    "feasibility": "solo-90-day",
    "wheelRisk": 6,
    "problem": "Facilities with solar panels and battery storage struggle to manually decide when to use stored energy versus grid power, especially with variable rates and intermittent generation. Suboptimal decisions can lead to higher costs or reliability issues. A smarter system is needed to automate energy management in real-time.",
    "approach": "Develop an AI controller that continuously monitors solar output, battery levels, load demand, and electricity prices. Using predictive algorithms (e.g., forecasting next hour’s solar generation and usage), it decides optimal charge/discharge schedules and grid usage. The system can send control signals to the battery inverter and other controllable loads. A web dashboard will show performance, savings, and allow overrides or mode changes by facility managers.",
    "oneWeek": "Scope Sprint: Model a simple scenario in code (e.g., one day of solar production vs load) and implement a rule-based optimizer. Show potential savings on historical data for a small commercial facility with a given solar/battery setup. Outline the AI approach (reinforcement learning or predictive control) to improve upon the rule-based strategy.",
    "ninetyDay": "Implement the AI optimization module using TensorFlow (or another ML framework) to learn from data and adjust control policies. Connect to a simulated or actual microgrid controller via Node.js backend and AWS Lambda for event-driven control actions. Test in a lab environment or simulation, demonstrating automatic cost-saving decisions (like peak shaving, shifting to battery during high tariff periods). Develop the UI for monitoring real-time status and override controls.",
    "risks": [
      "Unforeseen edge cases leading to power outages or equipment stress",
      "Model accuracy for predictions (weather, load) affects performance",
      "Cybersecurity of automated control signals"
    ],
    "buyVsBuild": "Consider: existing microgrid controllers from companies like Schneider or Siemens that offer optimization. Those are reliable but may not leverage cutting-edge AI or specific site customization. Build if the project requires a tailored approach or to integrate with unique site conditions and to avoid vendor lock-in.",
    "emb": [
      0.5,
      0.1,
      1.0,
      0.1,
      0.5,
      0.1,
      1.0,
      0.6
    ]
  },
  {
    "id": "ecoaudit-ai-advisor",
    "title": "EcoAudit AI Advisor",
    "sector": "Energy",
    "tags": [
      "energy-audit",
      "benchmarking",
      "retrofit",
      "anomaly-detection",
      "savings-estimator"
    ],
    "oneLiner": "AI-powered system that analyzes building energy usage patterns and recommends efficiency improvements and retrofits",
    "stack": [
      "Python",
      "Pandas",
      "scikit-learn",
      "React",
      "Plotly"
    ],
    "kpis": [
      "Identified 20% average energy savings per audit",
      "Auditing process time reduced by 40%",
      "Customized retrofit recommendations generated"
    ],
    "feasibility": "solo-90-day",
    "wheelRisk": 4,
    "problem": "Manual energy audits require substantial time from experts to comb through utility data, building information, and identify inefficiencies. This limits how many buildings can be audited and may miss subtle patterns. An automated assistant could rapidly analyze data and suggest improvements, augmenting the experts’ capabilities.",
    "approach": "Develop an AI assistant that takes in building data: smart meter readings, HVAC schedules, insulation levels, etc. Use machine learning to detect anomalies (like unusually high nighttime consumption) and compare performance against benchmarks or simulation models. The system then outputs a report of recommended actions (for example, \"upgrade lighting to LED\" or \"adjust thermostat settings after hours\") with estimated savings. The interface allows an energy engineer to input building specifics and get preliminary recommendations, accelerating the audit process.",
    "oneWeek": "Scope Sprint: Collect sample data from one building (e.g., a small office) including hourly energy usage and basic building info. Implement a simple rule-based analysis (if usage after 10pm is above X, flag HVAC schedule issue). Generate a mock report to illustrate recommendations. Identify data points needed for more advanced analysis.",
    "ninetyDay": "Integrate multiple data sources (e.g., weather, occupancy) for context. Train ML models to classify usage patterns and predict potential savings from various interventions. Build the React frontend for users to upload data (CSV or via API integration with smart meters) and view the generated audit report. Validate recommendations against a professional auditor’s assessment on a pilot building, refining accuracy and usefulness. Prepare case study results showing time saved and recommendations quality.",
    "risks": [
      "Limited data availability or quality for older buildings",
      "Recommendations might not account for all real-world constraints, leading to skepticism",
      "Need to regularly update models as building usage patterns change"
    ],
    "buyVsBuild": "Consider: existing energy management software (e.g., EnergyStar Portfolio Manager or commercial tools like SkySpark, BuildingIQ) for tracking usage. Those provide data views but often lack automated recommendation engines. Build if aiming for a proprietary AI that differentiates HDR’s audit services or integrates deeply with our project data and provides a unique selling point to clients.",
    "emb": [
      0.1,
      0.7,
      0.7,
      0.9,
      0.4,
      0.6,
      0.1,
      0.7
    ]
  },
  {
    "id": "renewsite-planner",
    "title": "RenewSite Planner",
    "sector": "Energy",
    "tags": [
      "siting",
      "solar",
      "wind",
      "interconnection",
      "suitability",
      "financial-model"
    ],
    "oneLiner": "Interactive GIS tool to evaluate potential solar/wind farm sites, calculating energy output and financial return for each location",
    "stack": [
      "Next.js",
      "Mapbox GL",
      "PostgreSQL/PostGIS",
      "Python",
      "NREL APIs"
    ],
    "kpis": [
      "Site analysis time cut by 50%",
      "Increased proposal success rate with data-backed site selection",
      "Accurate energy yield estimates (±5%)"
    ],
    "feasibility": "solo-90-day",
    "wheelRisk": 4,
    "problem": "Developers and engineers must assess numerous factors when choosing sites for renewable energy projects: resource availability (sun, wind), land suitability, grid access, and financial incentives. Doing this analysis manually for each potential site is slow and may result in suboptimal choices or missed opportunities, especially under tight proposal deadlines.",
    "approach": "Build a web-based mapping application where users can draw or select a site and immediately get an analysis. Integrate data layers such as solar irradiance maps, wind speed data, land use/parcel info, distance to nearest grid interconnection, and available incentives by region. The backend uses these inputs to estimate potential energy output (using models like NREL’s PVWatts for solar) and calculates an expected financial model (CAPEX, OPEX, payback period). The result is a ranked list of sites or a detailed report for a chosen site to support decision-making and client proposals.",
    "oneWeek": "Scope Sprint: Assemble key datasets for one region (e.g., solar irradiance and land use for a specific state). Create a simple map with Mapbox allowing selection of a test site polygon. Manually run a PVWatts calculation for that site to demonstrate energy yield output. Sketch out the financial model assumptions (cost per MW, etc.).",
    "ninetyDay": "Implement automated data retrieval for any user-selected location (e.g., query a solar API for irradiance and a wind database for speeds). Complete the financial calculations within the app, allowing the user to adjust parameters like system size or cost assumptions. Ensure the PostGIS/PostgreSQL database efficiently stores spatial data and query results for reuse. Add an export function to generate a summary report. Beta test with a renewable energy team on a few prospective sites and refine accuracy of predictions and usability of the interface.",
    "risks": [
      "Data resolution or errors could mislead site selection",
      "Rapid changes in incentives or energy prices require frequent updates",
      "Users may need training to interpret results correctly"
    ],
    "buyVsBuild": "Consider: NREL’s tools (e.g., REopt and PVWatts) which provide underlying calculations but not a full decision platform; also GIS systems like ArcGIS have some analysis capabilities. If those prove insufficient or not integrated, building a unified tool allows HDR to quickly iterate features and own the IP for a competitive edge in renewable consulting.",
    "emb": [
      0.7,
      0.8,
      0.1,
      0.0,
      0.9,
      0.5,
      0.8,
      0.4
    ]
  },
  {
    "id": "spacesense-occupancy",
    "title": "SpaceSense Occupancy Analytics",
    "sector": "Buildings",
    "tags": [
      "occupancy",
      "HVAC-optimization",
      "BMS",
      "privacy",
      "utilization-analytics"
    ],
    "oneLiner": "Platform that tracks and analyzes building occupancy in real-time to optimize space utilization and HVAC energy use",
    "stack": [
      "Node.js",
      "MQTT",
      "InfluxDB",
      "Grafana",
      "BACnet"
    ],
    "kpis": [
      "Real-time occupancy tracking implemented",
      "Office space utilization improved by 20%",
      "HVAC runtime reduced by 15% during off-hours"
    ],
    "feasibility": "solo-90-day",
    "wheelRisk": 4,
    "problem": "Many offices and facilities waste energy conditioning or lighting unoccupied spaces, and organizations often lease more space than needed because they lack data on actual usage. Without real-time occupancy insights, it's hard to make informed decisions about space planning and energy management.",
    "approach": "Set up motion sensors, badge readers, or Wi-Fi analytics to collect occupancy data for different zones in a building. Stream this data via MQTT to a central database. The platform visualizes current occupancy on floor maps and analyzes historical trends. It can trigger HVAC or lighting adjustments (e.g., turn off AC in empty areas) through integration with building management systems. The data also helps facility managers identify underutilized spaces and consolidate or reassign areas, leading to cost savings.",
    "oneWeek": "Scope Sprint: Instrument one conference room with a simple occupancy sensor. Connect the sensor data to a PostgreSQL database through an MQTT broker. Build a Grafana dashboard that shows when the room is occupied in real-time and logs daily utilization hours. Use this as a demo to gather requirements from facility stakeholders.",
    "ninetyDay": "Expand the deployment to a whole floor or small building. Implement a user-friendly web interface for live floorplan views of occupancy. Integrate with HVAC controls (perhaps via an API or BACnet gateway) to automatically adjust settings based on occupancy rules. Conduct a pilot test over a month to measure energy savings and get feedback from facility managers on the space usage insights. Iterate on sensor calibration and data accuracy as needed.",
    "risks": [
      "Privacy concerns with monitoring occupancy",
      "Sensor inaccuracies or failures leading to wrong automation triggers",
      "Integration challenges with existing Building Management Systems"
    ],
    "buyVsBuild": "Consider: commercial occupancy analytics solutions like VergeSense or Density, which offer hardware and analytics but could be costly at scale. Build if leveraging existing sensors (badge data, Wi-Fi) and for a tailored analytics approach that integrates with HDR’s specific building systems or client facilities on a budget.",
    "emb": [
      0.7,
      1.0,
      0.7,
      0.7,
      0.3,
      0.8,
      0.7,
      0.9
    ]
  },
  {
    "id": "twinview-facility-manager",
    "title": "TwinView Facility Manager",
    "sector": "Buildings",
    "tags": [
      "digital-twin",
      "BIM",
      "IoT",
      "CMMS",
      "alerts"
    ],
    "oneLiner": "3D digital twin of a building integrating live sensor data for proactive maintenance and streamlined facility operations",
    "stack": [
      "Autodesk Forge Viewer",
      "Azure IoT Hub",
      "Azure Digital Twins",
      "Node.js",
      "MongoDB"
    ],
    "kpis": [
      "Issue diagnosis time reduced by 30%",
      "Improved maintenance scheduling efficiency",
      "Single source of truth for building data established"
    ],
    "feasibility": "configure",
    "wheelRisk": 6,
    "problem": "Facility managers often juggle BIM models, sensor dashboards, and maintenance logs separately. Vital information about building systems is siloed, making it hard to visualize and understand the real-time state of the facility in context. This can delay troubleshooting and lead to suboptimal operational decisions.",
    "approach": "Create an interactive digital twin by linking the building’s 3D BIM model with real-time data feeds (HVAC performance, temperatures, equipment status). Using Autodesk Forge or similar, the BIM model is accessible in a web interface. Overlay sensor readings and alerts directly on the 3D model (e.g., a chiller flashing if a fault is detected). Maintenance personnel can click on any building component to see its specs, maintenance history, and live data. This integrated view helps quickly pinpoint issues and assess impacts. The pilot would use an existing HDR office or client building as a test case.",
    "oneWeek": "Scope Sprint: Obtain a BIM model (or floor plan) of the pilot building. Hook up one data source (e.g., a thermostat temperature) into a simple web page that shows the value when a room is clicked. Demonstrate the concept of linking model geometry to sensor data to facility stakeholders and IT for feedback on security/integration.",
    "ninetyDay": "Stand up the full digital twin environment using Autodesk Forge for model viewing and Azure IoT for data ingestion. Map several critical data streams into the model (e.g., HVAC readings, energy meters). Implement a basic alert system highlighting anomalies on the model. Train the facility management team on using the interface. Collect feedback on usability and identify additional features like AR mode or mobile access for future improvements. Document any efficiency gains observed during the pilot (e.g., faster identification of a failing component).",
    "risks": [
      "Data integration complexity with proprietary building systems",
      "Ensuring the digital model stays updated with physical changes",
      "User adoption – facilities staff may resist new tools without clear benefits"
    ],
    "buyVsBuild": "Consider: comprehensive facility management suites (IBM Maximo, etc.) and newer digital twin platforms like Azure Digital Twins or Siemens Navigator. These offer frameworks but may not mesh with existing BIM data out-of-the-box. Configure using platforms like Forge for a custom-fit solution that leverages our in-house BIM expertise and targets the client’s specific needs.",
    "emb": [
      0.3,
      0.2,
      1.0,
      0.8,
      0.5,
      0.9,
      0.1,
      0.7
    ]
  },
  {
    "id": "sitevision-progress",
    "title": "SiteVision Progress Monitor",
    "sector": "Buildings",
    "tags": [
      "construction",
      "progress-tracking",
      "BIM-compare",
      "drone",
      "schedule-variance",
      "QA/QC"
    ],
    "oneLiner": "Uses drone imagery and AI to compare construction site progress against BIM models and schedules, flagging delays and quality issues early",
    "stack": [
      "Python",
      "OpenCV",
      "PyTorch",
      "AWS S3",
      "Three.js"
    ],
    "kpis": [
      "Detection of schedule deviations within 24 hours",
      "Reduced site inspection hours by 30%",
      "Improved issue resolution time"
    ],
    "feasibility": "solo-90-day",
    "wheelRisk": 7,
    "problem": "Construction managers rely on manual site walks and reports to assess progress, which can be subjective and infrequent. This means schedule slips or errors in construction might go unnoticed for weeks. There’s a need for an objective, frequent, and automated way to verify progress and catch issues early by comparing what’s built on-site to the plan.",
    "approach": "Utilize drone or 360° camera imagery captured regularly on-site and process it with computer vision. The system will align the images to the 3D BIM model or previous images and use AI to identify completed vs pending work (e.g., walls constructed, equipment installed). It can also highlight anomalies, like an installed element that doesn’t match the model or safety hazards visible in imagery. A web dashboard will show a visual diff of planned vs actual and list potential issues or delays, so project managers can act quickly.",
    "oneWeek": "Scope Sprint: Use a set of sample construction site photos and the corresponding portion of a BIM model. Develop a basic OpenCV script to compare two images taken a week apart, highlighting differences (new vs missing objects). Manually correlate these differences to construction activities to show feasibility. Prepare a concept demo for the project team to get their input.",
    "ninetyDay": "Train a PyTorch model on recognizing certain construction elements (e.g., detect presence of structural framing or drywall). Set up a pipeline where new drone images are uploaded to AWS S3 and automatically processed. Build a simple UI to visualize detected progress (e.g., color-coding areas of the model that are completed). Pilot this on an active project by running the system in parallel with traditional progress tracking, and compare findings. Adjust the AI model for site-specific conditions and document any caught issues that would have been missed by standard practice.",
    "risks": [
      "Accuracy of computer vision in messy, changing environments",
      "Dependence on regular image capture (drones or cameras need scheduling)",
      "Potential resistance from site teams wary of AI judgments"
    ],
    "buyVsBuild": "Consider: startups like Doxel or OpenSpace that offer automated progress tracking with proprietary hardware and software. These solutions are effective but can be expensive and might not integrate with our BIM workflows. Build if we want to develop internal capability, especially to integrate with our existing project management systems and tailor the detection to focus on our priorities (e.g., specific quality checks).",
    "emb": [
      0.6,
      0.4,
      0.7,
      0.8,
      0.2,
      0.8,
      0.1,
      0.7
    ]
  },
  {
    "id": "cv-site-safety",
    "title": "Computer Vision Site Safety Monitor",
    "sector": "Buildings",
    "tags": [
      "safety",
      "PPE",
      "zone-breach",
      "near-miss",
      "computer-vision",
      "hardhat",
      "hi-vis-vest"
    ],
    "oneLiner": "Detect PPE compliance, zone breaches, and near-misses from site cameras with a human review queue",
    "stack": [
      "Python",
      "PyTorch",
      "OpenCV",
      "FastAPI",
      "RTSP",
      "AWS S3"
    ],
    "kpis": [
      ">90% precision on PPE/zone events (pilot)",
      "Reduce manual video review time by 50%",
      "Actionable alerts under 60 seconds"
    ],
    "feasibility": "solo-90-day",
    "wheelRisk": 1,
    "problem": "Site teams miss PPE violations and restricted-area intrusions when relying on manual monitoring and periodic walks. Near-misses are under-reported, reducing the ability to learn and prevent incidents.",
    "approach": "Ingest RTSP camera feeds, run on-edge or server CV models for PPE (hardhat, vest) and zone intrusion. Queue detections for human review, suppress duplicates, and export events to safety logs. Start with one cam + two rules, then expand.",
    "oneWeek": "Set up a single camera stream and run pre-trained PPE/zone models on recorded clips. Produce a web preview with bounding boxes and a simple review/approve queue.",
    "ninetyDay": "Deploy to 3–5 cameras on one site, add zone polygons, alerting (email/SMS), analytics (trends by area/time), and export to existing safety workflows. Conduct precision/recall validation with the safety manager.",
    "risks": [
      "Privacy and consent for video analytics",
      "False positives in complex scenes",
      "Network/camera reliability on active sites"
    ],
    "buyVsBuild": "Consider: Intenseye/ViViD/Triax for turnkey PPE analytics. Build to tailor rules, integrate with BIM/schedules, and keep footage on client-controlled infra.",
    "emb": [
      0.72,
      0.18,
      0.65,
      0.31,
      0.84,
      0.22,
      0.47,
      0.59
    ]
  },
  {
    "id": "forma-carbon-tracker",
    "title": "AI Carbon Design (Autodesk Forma)",
    "sector": "Environmental",
    "tags": [
      "Forma",
      "embodied-carbon",
      "operational-carbon",
      "early-phase",
      "massing",
      "design-opt"
    ],
    "oneLiner": "Early-phase massing checks for embodied/operational CO₂ that suggest lower‑carbon options via Autodesk Forma integration",
    "stack": [
      "Next.js",
      "Python",
      "Pandas",
      "DuckDB",
      "PostgreSQL",
      "Chart.js"
    ],
    "kpis": [
      "30% reduction in reporting time",
      "Real-time emission tracking",
      "Compliance dashboard"
    ],
    "feasibility": "solo-90-day",
    "wheelRisk": 1,
    "problem": "Organizations struggle to track carbon emissions across multiple facilities and supply chains. Manual data collection is error-prone and time-consuming. Compliance reporting requires significant manual effort.",
    "approach": "Build a dashboard that ingests utility bills and operational data, applies emission factors, and generates visualizations. Use pre-built carbon calculation libraries and focus on UX for data input and report generation.",
    "oneWeek": "Scope Sprint: Define data sources (utility bills, transportation logs). Prototype a single-facility dashboard mockup. Identify 3 key emission categories to track. Map out data flow from input to visualization.",
    "ninetyDay": "Deliver MVP with multi-facility tracking, CSV import, automated monthly reports, and basic reduction recommendations. Include compliance export templates for common frameworks (GHG Protocol, CDP).",
    "risks": [
      "Data quality depends on manual input",
      "Emission factor databases may require licensing",
      "Scope creep into supply chain tracking"
    ],
    "buyVsBuild": "Consider: Watershed, Persefoni (enterprise focus, $$$). Build if you need customization for specific industry or internal integration with existing systems.",
    "emb": [
      0.0,
      0.1,
      0.2,
      0.3,
      0.2,
      0.8,
      0.6,
      0.2
    ]
  },
  {
    "id": "ecopermitting-ai-assistant",
    "title": "EcoPermitting AI Assistant",
    "sector": "Environmental",
    "tags": [
      "NEPA",
      "permitting",
      "NLP",
      "wetlands",
      "checklists",
      "jurisdiction"
    ],
    "oneLiner": "AI assistant that streamlines environmental permitting by identifying required permits and compliance steps for projects across jurisdictions",
    "stack": [
      "Python",
      "spaCy",
      "Elasticsearch",
      "PostgreSQL",
      "OpenAI API",
      "Next.js"
    ],
    "kpis": [
      "50% faster permit identification process",
      "Reduced errors in permit applications",
      "Comprehensive regulatory coverage (local, state, federal)"
    ],
    "feasibility": "solo-90-day",
    "wheelRisk": 5,
    "problem": "Environmental permitting for projects is complex and varies by location and project type. Project teams must manually research which environmental permits and reviews (NEPA, wetlands, air quality, etc.) apply, which is time-consuming and prone to missing less obvious requirements, risking compliance issues or delays.",
    "approach": "Develop an AI-driven tool where a user inputs key project details (location, project type, size, environmental impact factors). The system uses an NLP engine trained on regulations to suggest a list of likely required permits and studies. It provides references to the specific regulation text for each permit and a checklist of steps to comply. The tool could also track status of each permit through completion. Start with a specific region or subset of regulations to train the model and demonstrate value, then expand.",
    "oneWeek": "Scope Sprint: Focus on one permit domain (e.g., wetland permitting). Gather a dataset of related regulations and permit guidelines. Build a simple rule-based prototype: if project involves wetlands and >acre threshold, then Clean Water Act Section 404 permit needed. Show how input parameters map to a basic permit list to illustrate the concept.",
    "ninetyDay": "Expand to a broader set of regulations. Implement an NLP model (using spaCy or similar) to extract rules from text or match project descriptions to requirements. Create an Angular frontend for users to enter project info and get results. Integrate ElasticSearch to efficiently query a database of regulations and past project permit data. Test with a few sample projects and compare the AI’s recommended permit list with what experts would identify, refining accuracy. Prepare a case for how this tool could reduce research time and ensure compliance completeness on complex projects.",
    "risks": [
      "Keeping regulatory data up-to-date",
      "False confidence in AI suggestions if regulations interpreted incorrectly",
      "Customization needed for different regions or sectors"
    ],
    "buyVsBuild": "Consider: legal database services or environmental compliance management software that include permit tracking, but they often lack intelligent suggestion features. Build if aiming to encapsulate HDR’s deep regulatory knowledge into a tool, giving us an internal advantage and potential client-facing product. This could also reduce reliance on individual heroics for complex permitting processes.",
    "emb": [
      0.7,
      0.0,
      0.1,
      0.3,
      0.9,
      0.7,
      0.1,
      0.1
    ]
  },
  {
    "id": "ai-proposal-verification",
    "title": "AI Proposal Verification Assistant",
    "sector": "Internal",
    "tags": [
      "proposal-QA",
      "compliance",
      "RFP/RFQ",
      "consistency",
      "risk-flags",
      "style-guide",
      "non-generative"
    ],
    "oneLiner": "LLM-assisted checker that flags RFP compliance gaps, inconsistent data, risky claims, and style issues in proposals—no generative text in final deliverables",
    "stack": [
      "Next.js",
      "Python",
      "spaCy",
      "OpenAI API",
      "Elasticsearch",
      "PostgreSQL"
    ],
    "kpis": [
      "30% faster internal review cycle",
      "Zero critical compliance misses in pilot proposals",
      "50% reduction in consistency/formatting errors"
    ],
    "feasibility": "solo-90-day",
    "wheelRisk": 1,
    "problem": "Proposal teams spend significant time manually checking RFP/RFQ compliance, cross-referencing requirements, and hunting for inconsistencies (names, dates, project numbers, resumes, certifications). Manual review is error-prone, and HDR leadership has stated that generative AI should not produce final deliverables, limiting typical 'AI writing' tools.",
    "approach": "Build a verification-first pipeline: (1) ingest DOCX/PDF and segment by sections; (2) rule engine runs must-have checks derived from RFP (e.g., required forms, page limits, key personnel, insurance limits); (3) LLM classifiers (analysis-only) rate compliance, detect risky claims/unsupported metrics, and spot contradictions; (4) consistency checks for names/dates/figures across the document set; (5) style/branding linting against a configurable guide; (6) PII scan and redaction suggestions. Output a prioritized issue list with severity, page/section citations, and suggested fixes (templated, not generative).",
    "oneWeek": "Scope Sprint: Select 2–3 redacted past proposals + one target RFP. Define 10–15 high-value checks (compliance, page limits, key staff, insurance, forms). Build a CLI prototype that parses one DOCX/PDF, runs rules + basic LLM classification, and outputs a JSON issue list. Create a simple web mock of the issue dashboard.",
    "ninetyDay": "Deliver a web tool where staff upload proposal files, run the verification pipeline, and review a dashboard of issues with filters (severity, category). Include export to PDF/CSV, audit log of fixes, and a 'Ready to Submit' checklist. Integrate read-only access to a SharePoint sample library (if available) to reuse references; no generative text in outputs. Validate on 5–10 proposals and report time saved + zero critical misses.",
    "risks": [
      "False positives/negatives may erode trust—mitigate with human-in-the-loop review",
      "Handling sensitive client data—enforce access controls and PII redaction",
      "Parsing variability across complex PDFs—fallback to DOCX where possible"
    ],
    "buyVsBuild": "Consider: RFPIO/Loopio/QorusDocs offer authoring and basic checks; some legal AI (e.g., Document Crunch) flags clauses but not A/E proposal specifics. Build to add deep RFP rule packs, cross-document consistency validation, and strict 'no-gen' compliance aligned to HDR policy.",
    "emb": [
      0.4,
      0.9,
      0.2,
      0.7,
      0.5,
      0.3,
      0.8,
      0.6
    ]
  }
]